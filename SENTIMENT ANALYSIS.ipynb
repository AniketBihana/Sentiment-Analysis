{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec756588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt|\n",
    "\n",
    "\n",
    "# nltk\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#spacy\n",
    "\n",
    "import spacy\n",
    "!pip install spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#model building\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the file\n",
    "df = pd.read_csv(\"E:\\\\NLP PROJECTS\\\\extra\\\\twitter_training.csv\",encoding='ISO-8859-1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving columns names\n",
    "#bot means base of  topic\n",
    "df.columns = ['Tweet_id','BOT','Sentiments','Tweet_content']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36eef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping null values rows and renaming the new data columns\n",
    "df1 =df.dropna(axis=0)\n",
    "df1.columns = ['Tweet_id','BOT','Sentiments','Tweet_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6afdb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rechecking null values\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1 lowercasing all elements\n",
    "df1['clean_text'] = df1['Tweet_content'].apply(lambda x: x.lower())\n",
    "\n",
    "#2 removing urls\n",
    "def remove_urls(text):\n",
    "    return re.sub('https.............\\S+', '', text)\n",
    "    return re.sub('bit.......\\S+' '', text)\n",
    "\n",
    "df1['clean_text'] = df1['Tweet_content'].apply(remove_urls)\n",
    " \n",
    "    \n",
    "    #3 removing special characters and puntuations\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "df1['clean_text'] = df1['Tweet_content'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501903a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define contractions\n",
    "contractions = {\n",
    "    \"im\" : \"i am\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"we'll\": \"we will\"\n",
    "    \n",
    "}\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_text(text):\n",
    "    # Join the list of tokens into a single string\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    # Remove contractions\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = re.sub(r'\\b' + contraction + r'\\b', expansion, text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize the remaining tokens using spaCy\n",
    "    doc = nlp(\" \".join(filtered_tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Join the lemmatized tokens into a string\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply the processing function to the 'clean_text' column of your DataFrame\n",
    "df1['clean_text'] = df1['clean_text'].apply(process_text)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['clean_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a702e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extracting tokenized clean_text column\n",
    "tokenized_clean_text_data = df1['clean_text'].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_clean_text_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create word vectors\n",
    "word_vectors = word2vec_model.wv\n",
    "\n",
    "\n",
    "# Convert tokenized data to word vectors\n",
    "word_vector_data = []\n",
    "for text in tokenized_clean_text_data:\n",
    "    vectors = [word_vectors[word] for word in text if word in word_vectors]\n",
    "    if vectors:\n",
    "        document_vector = sum(vectors) / len(vectors)\n",
    "        word_vector_data.append(document_vector)\n",
    "    else:\n",
    "        # Handle empty text case\n",
    "        word_vector_data.append([0] * word2vec_model.vector_size)\n",
    "\n",
    "# Convert document vectors to DataFrame (optional)\n",
    "document_vectors_df = pd.DataFrame(word_vector_data)\n",
    "print(document_vectors_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ebf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler to data and transform\n",
    "document_vectors_normalized = scaler.fit_transform(document_vectors_df)\n",
    "\n",
    "# Now 'document_vectors_normalized' contains normalized vectors with values between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef3b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extracting clean_text column\n",
    "clean_text_data = df1['clean_text'].tolist()\n",
    "labels = df1['Sentiments']  \n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')  # You can change the kernel as needed\n",
    "svm_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predictions using SVM model\n",
    "svm_predictions = svm_model.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate accuracy for SVM model\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Calculate F1 score for SVM model\n",
    "svm_f1_score = f1_score(y_test, svm_predictions, average='weighted')\n",
    "print(\"SVM F1 Score:\", svm_f1_score)\n",
    "\n",
    "# Confusion matrix for SVM model\n",
    "svm_confusion_matrix = confusion_matrix(y_test, svm_predictions)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(svm_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting clean_text column and labels\n",
    "clean_text_data = df1['clean_text'].tolist()\n",
    "labels = df1['Sentiments']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Define the SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto']  # Only relevant for 'rbf' and 'poly' kernels\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Predict using the best estimator\n",
    "svm_predictions = best_estimator.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate accuracy for the best estimator\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Calculate F1 score for the best estimator\n",
    "svm_f1_score = f1_score(y_test, svm_predictions, average='weighted')\n",
    "print(\"SVM F1 Score:\", svm_f1_score)\n",
    "\n",
    "# Confusion matrix for the best estimator\n",
    "svm_confusion_matrix = confusion_matrix(y_test, svm_predictions)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(svm_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf98a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964312e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
